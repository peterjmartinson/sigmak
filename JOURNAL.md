## 12/25/2025

Issue #1
Setup the core directory structure, initialize the uv project, and establish the TDD framework.

## [2025-12-27] Milestone: The Ingestion Inception (Issue #1)

### Status: COMPLETED

### Summary:
Successfully built the "Walking Skeleton" for the SEC filing ingestion engine. The primary challenge was the tight coupling between file operations and parsing logic, which made testing brittle. By splitting these concerns, we now have a "pure" parser that can be tested against mock SEC strings without touching the disk.

### Technical Decisions:
* **Atomic over Monolithic:** Abandoned the single "kitchen-sink" test in favor of granular assertions. This allows for faster debugging when SEC document structures shift.
* **Encoding Resilience:** Implemented a fallback to `CP1252` after observing that many older SEC filings do not strictly adhere to `UTF-8`.
* **In-Memory Mocking:** Utilized `pytest` fixtures and `tmp_path` to simulate filing downloads, ensuring the dev environment remains clean.

### Lessons Learned:
* Don't trust the disk: Always have a logic path that accepts a string directly.
* The "Combat Aborted" pattern (manual file checking) is a speed-killer; automated temporary fixtures are the way forward.

### Next Steps:
* [ ] Issue #2: Target Section Extraction (Regex/DOM logic for "Item 1A").
* [ ] Issue #3: Implement text chunking logic for the RAG pipeline.

## [2025-12-28] Subissue 1.0: Recursive Chunking Integrated

### Technical Summary
Successfully implemented the "Transform" phase of the ingestion pipeline. Raw extraction now flows into structured, metadata-tagged atoms.
- **Strategy**: Used `RecursiveCharacterTextSplitter` from `langchain-text-splitters`.
- **Parameters**: `chunk_size=800`, `chunk_overlap=80`. These were chosen to balance semantic density with LLM context window efficiency.
- **Separators**: Configured to respect SEC filing structure (double-newlines first, then single-newlines, then sentence boundaries).

### Performance Metrics
- **Avg. Chunks per 10-K**: [Insert number from your run, e.g., 42 chunks].
- **Processing Latency**: [Insert time, e.g., <100ms] (Local CPU processing).

### Observations
- Moving to `langchain-text-splitters` instead of the full `langchain` package kept the environment footprint significantly smaller.
- The unit tests confirmed that metadata (ticker, year) is correctly propagated to every atomic chunk.

## [2025-12-30] Subissue 1.1: Chroma DB Infrastructure Integrated

### Technical Summary
Established the "Vault" layer of the pipeline, moving from ephemeral text processing to a persistent semantic storage engine.

Strategy: Implemented a PersistentClient in Chroma DB to ensure the vector index survives session restarts on the WSL disk.

Index Configuration: Created the sec_risk_factors collection using hnsw:space: "cosine". This distance metric was chosen specifically for SEC filings, as it prioritizes the thematic orientation of risk disclosures over raw document length.

Idempotency: Switched to get_or_create_collection to support "Onion Stability," allowing the script to initialize an empty DB or reconnect to an existing one without data duplication.

### Performance Metrics
Cold Start Latency: ~1.2s (Client initialization and HNSW graph creation).

Heartbeat Latency: <1ms (Verified via client.heartbeat()).

Disk Footprint: ~64KB (Base SQLite metadata overhead before ingestion).

### Observations
Relational Parallels: The transition was smoothed by treating "Collections" as Tables and "Metadata" as indexed WHERE clauses.

TDD Rigor: Breaking the "Combat Test" into four discrete units (Persistence, Heartbeat, Collection Integrity, and Type Integrity) allowed for much faster mypy validation.

Git Hygiene: Explicitly added chroma_db/ to .gitignore to prevent binary bloat in the repository while keeping the ingestion pipeline reproducible.

Updated Milestones
[x] Issue #2: Walking Skeleton / Inception.

[x] Subissue 2.0: Recursive Chunking logic.

[x] Subissue 2.1: Chroma DB Infrastructure.

[ ] Subissue 2.2: Embedding Generation (Integrating the LLM transformer).

Next Step
With the infrastructure live and the journal updated, we are ready for Subissue 1.2. This is where we choose our "Brain"â€”the model that turns your SEC chunks into vectors.
